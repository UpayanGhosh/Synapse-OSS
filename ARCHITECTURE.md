# ğŸ§  System Architecture â€” Project Phoenix

> A deep-dive into the modular, decentralized, and self-evolving design of JARVIS.

GitHub automatically renders the Mermaid diagrams below. If you are viewing this locally, use a Markdown viewer that supports Mermaid.js, or view it on GitHub.

---

## Architecture Diagram

![JARVIS â€” Project Phoenix Architecture](./architecture_diagram.png)

> *Full interactive diagram with annotations is available in the [Figma file](https://www.figma.com/@upayan). The sections below break down each subsystem in detail.*

---

## High-Level System Map

This diagram illustrates the full end-to-end flow: from user input, through the Async Gateway Pipeline, across the Cognitive Engine (MoA + Dual Cognition), and back out as a response.

```mermaid
graph TD
    %% â”€â”€ Styling â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    classDef user      fill:#2d3436,stroke:#74b9ff,stroke-width:2px,color:#fff
    classDef gateway   fill:#0984e3,stroke:#74b9ff,stroke-width:3px,color:#fff
    classDef async     fill:#00cec9,stroke:#81ecec,stroke-width:2px,color:#000
    classDef memory    fill:#00b894,stroke:#55efc4,stroke-width:2px,color:#fff
    classDef sbs       fill:#fdcb6e,stroke:#f39c12,stroke-width:2px,color:#000
    classDef moa       fill:#6c5ce7,stroke:#a29bfe,stroke-width:2px,color:#fff
    classDef local     fill:#d63031,stroke:#ff7675,stroke-width:2px,color:#fff

    %% â”€â”€ Ingress â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    U1[ğŸ“± WhatsApp Webhook\nNode Gateway]:::user -->|HTTP POST /webhook| FG
    U2[ğŸ’» OpenClaw CLI\nProxy Request]:::user   -->|CLI Proxy| G

    %% â”€â”€ Async Pipeline â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    subgraph Async_Pipeline [Async Gateway Pipeline]
        FG{ğŸ›¡ï¸ FloodGate\nBatch Window 3s}:::async
        DD[ğŸ” MessageDeduplicator\n5-min window]:::async
        Q[(ğŸ“¦ TaskQueue\nmax 100)]:::async
        W[âš™ï¸ MessageWorker\n2 concurrent]:::async
        FG --> DD --> Q --> W
    end
    W --> G

    %% â”€â”€ Core Gateway â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    G((ğŸš€ Core API Gateway\nFastAPI / Uvicorn\n:8000)):::gateway

    %% â”€â”€ Soul-Brain Sync (SBS) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    subgraph SBS [Soul-Brain Sync â€” Persona Engine]
        SBS_O[ğŸ­ SBS Orchestrator\nthe_creator / the_partner]:::sbs
        SBS_P[ğŸ“‹ Profile Manager\nMood Â· Vocab Â· Sentiment]:::sbs
        SBS_L[ğŸ“ Conversation Logger\nSQLite per-session]:::sbs
        SBS_RT[âš¡ Realtime Processor\nSentiment Â· Language Â· Mood]:::sbs
        SBS_B[ğŸ”„ Batch Processor\nEvery 50 msgs or 6h]:::sbs
        SBS_C[ğŸ–Šï¸ Prompt Compiler\nSystem Prompt Assembly]:::sbs
        G <-->|Inject Persona Context| SBS_O
        SBS_O --- SBS_P --- SBS_L
        SBS_O --- SBS_RT --- SBS_B
        SBS_O --- SBS_C
    end

    %% â”€â”€ Memory Subsystem â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    subgraph Cognitive_Memory [Cognitive Memory â€” Hybrid RAG]
        ME[ğŸ§  Memory Engine\nHybrid Retrieval v3]:::memory
        M1[("ğŸ—ƒï¸ SQLite Graph DB\nTriples Â· knowledge_graph.db")]:::memory
        M2[(ğŸ”· Qdrant Vector DB\nnomic-embed-text\n:6333)]:::memory
        RE[("ğŸ… FlashRank Reranker\nms-marco-TinyBERT")]:::memory
        G <-->|Semantic + Graph Query| ME
        ME <--> M1
        ME <--> M2
        ME --> RE
    end

    %% â”€â”€ Dual Cognition â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    subgraph Dual_Cognition [Dual Cognition Engine]
        DC[ğŸ§© DualCognitionEngine\nInner Monologue Â· Tension Calc]:::memory
        TS[â˜£ï¸ LazyToxicScorer\nidle timeout 30s]:::memory
        G --> DC
        DC --- TS
    end

    %% â”€â”€ Mixture of Agents â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    subgraph Mixture_of_Agents [Mixture of Agents â€” MoA Router]
        TC{ğŸš¦ Traffic Cop\nIntent Classifier}:::moa
        G -->|Classify Intent| TC

        TC -->|CASUAL| LLM1[ğŸŸ¢ Gemini 3 Flash\nAG_CASUAL\nFast Â· Low Cost]:::moa
        TC -->|CODING| LLM2[("ğŸ’» The Hacker\nClaude Sonnet 4.5\nHigh Logic")]:::moa
        TC -->|ANALYSIS| LLM3[ğŸ›ï¸ The Architect\nGemini 3 Pro\nDeep Synthesis]:::moa
        TC -->|REVIEW| LLM4[("ğŸ§ The Philosopher\nClaude Opus 4.6\nCritique Â· Judgment")]:::moa
        TC -->|SPICY / Private| LLM5[("ğŸŒ¶ï¸ The Vault\nLocal Stheno on Ollama\nZero Cloud Leakage")]:::local
    end

    %% â”€â”€ Return Path â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    LLM1 & LLM2 & LLM3 & LLM4 & LLM5 -->|Response + Footer Stats| G
    G -->|Auto-Continue if cut-off| AC[âœ‚ï¸ Auto-Continue\nBackground Task]:::async
    G -->|Final Output| U1
    G -->|Final Output| U2
```

---

## Component Breakdown

### 1. ğŸ“± Ingress Layer

| Input Channel | Transport | Handler |
|---|---|---|
| WhatsApp (via Node Gateway) | HTTP POST `/webhook` | `FloodGate` â†’ `Deduplicator` â†’ `TaskQueue` |
| OpenClaw CLI | CLI Proxy subprocess | Direct â†’ `Core API Gateway` |

### 2. âš™ï¸ Async Gateway Pipeline (`workspace/sci_fi_dashboard/gateway/`)

Messages from WhatsApp enter an asynchronous multi-stage pipeline **before** hitting the cognitive engine. This prevents webhook timeouts and ensures ordered, deduplicated processing.

```mermaid
sequenceDiagram
    participant WA as WhatsApp Node
    participant FG as FloodGate (3s batch)
    participant DD as Deduplicator (5m)
    participant Q  as TaskQueue (100)
    participant W  as MessageWorker (Ã—2)
    participant G  as API Gateway

    WA->>FG: POST /whatsapp/enqueue
    FG->>FG: Batch rapid-fire messages
    FG->>DD: Flush batch
    DD->>DD: Check 5-min seen-set
    DD->>Q: Enqueue MessageTask
    Q->>W: Dequeue (FIFO)
    W->>G: process_message_pipeline()
    G-->>W: reply string
    W->>WA: send_via_cli()
```

| File | Role |
|---|---|
| `gateway/queue.py` | `TaskQueue` â€” asyncio-based FIFO, max 100 tasks |
| `gateway/flood.py` | `FloodGate` â€” batches messages within a 3-second window |
| `gateway/dedup.py` | `MessageDeduplicator` â€” 5-minute seen-set for exact deduplication |
| `gateway/worker.py` | `MessageWorker` â€” 2 concurrent async workers consuming the queue |
| `gateway/sender.py` | `WhatsAppSender` â€” wraps the OpenClaw CLI `send` command |

---

### 3. ğŸš€ Core API Gateway (`api_gateway.py`)

The central FastAPI application running on **port 8000**. Every cognitive operation is orchestrated from here.

**API Routes:**

| Method | Route | Description |
|---|---|---|
| `POST` | `/chat/the_creator` | Chat endpoint for primary user (brother mode) |
| `POST` | `/chat/the_partner` | Chat endpoint for partner (caring PA mode) |
| `POST` | `/chat` | Generic fallback (Banglish persona) |
| `POST` | `/whatsapp/enqueue` | Async WhatsApp ingress entry point |
| `GET`  | `/whatsapp/status/{id}` | Poll status of an enqueued message |
| `POST` | `/persona/rebuild` | Re-parse chat logs and rebuild persona profiles |
| `GET`  | `/persona/status` | Profile statistics and embedding mode |
| `POST` | `/ingest` | Ingest a structured fact into the knowledge graph |
| `POST` | `/add` | Unstructured memory â†’ LLM â†’ triple extraction |
| `POST` | `/query` | Query the knowledge graph |
| `GET`  | `/health` | System health check |
| `GET`  | `/v1/models` | OpenAI-compatible model list (for Node Gateway discovery) |
| `POST` | `/v1/chat/completions` | OpenAI-compatible proxy endpoint |

**Singleton Modules (initialized once at boot):**

```python
brain          = SQLiteGraph()           # Knowledge graph
gate           = EntityGate(...)         # FlashText keyword extractor
conflicts      = ConflictManager(...)    # Conflict deduplication
toxic_scorer   = LazyToxicScorer(...)    # Lazy-loaded toxicity scorer
memory_engine  = MemoryEngine(...)       # Hybrid RAG engine
dual_cognition = DualCognitionEngine(...)# Inner monologue engine
```

---

### 4. ğŸ§  Cognitive Memory â€” Hybrid RAG (`memory_engine.py`, `sqlite_graph.py`, `retriever.py`)

Three-tier retrieval engine that provides grounded memory context before any LLM call.

```mermaid
graph LR
    Q[User Query] --> EE[Entity Extraction\nFlashText]
    EE --> GQ[Graph Query\nSQLite Triples]
    EE --> VQ[Vector Search\nQdrant + nomic-embed-text]
    GQ --> MERGE[Score Merge\nÎ±=0.7 semantic + Î²=0.1 temporal]
    VQ --> MERGE
    MERGE --> FG2{High Confidence\n> 0.80?}
    FG2 -->|Yes| FAST[âš¡ Fast Gate\nReturn top-k directly]
    FG2 -->|No| RR[("ğŸ… FlashRank Reranker\nms-marco-TinyBERT")]
    RR --> OUT[Ranked Context\nfor Prompt]
    FAST --> OUT
```

| Store | Technology | Port | Purpose |
|---|---|---|---|
| `memory.db` | SQLite | local file | Document store & embedding queue |
| `knowledge_graph.db` | SQLite (graph) | local file | Subjectâ€“Predicateâ€“Object triple store |
| Qdrant | Qdrant (native binary) | `:6333` | High-speed semantic vector search |

**Retrieval Tiers:**

1. **Fast Gate** â€” if â‰¥ `limit` results score > 0.80, return immediately (no reranker overhead).
2. **Reranked** â€” `FlashRank` (ms-marco-TinyBERT-L-2-v2) re-scores all Qdrant candidates for higher precision.

**Temporal Routing:**

- Queries containing words like `"was"`, `"history"`, `"2024"` â†’ `Î²=0.0` (pure semantic, no recency boost).
- Queries containing `"current"`, `"now"`, `"today"` â†’ `Î²=0.5` (blend recency with semantic).
- Default â†’ `Î²=0.1` (mild recency nudge).

---

### 5. ğŸ­ Soul-Brain Sync (SBS) Persona Engine (`sbs/`)

The SBS system is responsible for making JARVIS feel like a person, not a chatbot. It continuously tracks and evolves a structured **persona profile** for each conversation target in real-time.

```mermaid
graph TD
    MSG[Inbound Message] --> RT[Realtime Processor\nSentiment Â· Language Â· Mood]
    RT --> LOG[Conversation Logger\nSQLite]
    LOG --> CNT{â‰¥ 50 msgs\nor 6h elapsed?}
    CNT -->|Yes| BATCH[Batch Processor\nProfile Rebuild]
    BATCH --> PM[(Profile Manager\nJSON Layers)]
    PM --> PC[Prompt Compiler\nSystem Prompt Assembly]
    PC --> SYS[Assembled System Prompt\nâ†’ LLM]
```

**Profile Layers tracked per target:**

| Layer | Data captured |
|---|---|
| `emotional_state` | Dominant mood, sentiment average, mood trajectory |
| `linguistic` | Banglish ratio, formality index, language mix |
| `vocabulary` | Unique word count, preferred phrases, emoji frequency |
| `meta` | Total messages processed, last batch run timestamp, profile version |

**Two SBS instances run simultaneously:**
- `sbs_the_creator` â€” tuned for primary user (casual, direct, sibling-like)
- `sbs_the_partner` â€” tuned for the partner (warm, supportive, PA-like)

---

### 6. ğŸ§© Dual Cognition Engine (`dual_cognition.py`)

Before generating a reply, JARVIS thinks. The `DualCognitionEngine` generates an **inner monologue** and calculates a **tension level** to decide if there is emotional conflict between the retrieved memory and the current user request.

```mermaid
graph LR
    UM[User Message] --> DC["DualCognitionEngine.think()"]
    DC --> IM[Inner Monologue\nvia Gemini Flash]
    DC --> TL[Tension Level\n0.0 â€“ 1.0]
    DC --> TT[Tension Type\nAMBIVALENT Â· CERTAIN Â· etc.]
    IM & TL & TT --> CC[Cognitive Context Block\ninjected into System Prompt]
```

The `LazyToxicScorer` is loaded alongside DualCognition. It auto-unloads after **30 seconds of idle** to save RAM â€” critical for a Mac Air host.

---

### 7. ğŸš¦ Mixture of Agents (MoA) Router

The **Traffic Cop** classifies every user message before routing it to the appropriate specialist model.

```mermaid
graph TD
    TC{Traffic Cop\nGemini Flash Classifier} -->|CASUAL| A[ğŸŸ¢ AG_CASUAL\nGemini 3 Flash\nHigh throughput / free tier]
    TC -->|CODING| B[ğŸ’» The Hacker\nClaude Sonnet 4.5\nMax logic depth]
    TC -->|ANALYSIS| C[ğŸ›ï¸ The Architect\nGemini 3 Pro\nLong-context synthesis]
    TC -->|REVIEW| D[ğŸ§ The Philosopher\nClaude Opus 4.6\nNuanced critique]
    TC -->|SPICY session| E[ğŸŒ¶ï¸ The Vault\nStheno v3.2 on Ollama\nZero cloud footprint]
```

All cloud models route through the **Antigravity Proxy** (`localhost:8080`) using an OAuth token. The vault (Stheno) connects directly to a Windows PC Ollama instance (`WINDOWS_PC_IP:11434`).

**Model constants (configurable via env):**

| Constant | Default Model |
|---|---|
| `MODEL_CASUAL` | `gemini-3-flash` |
| `MODEL_CODING` | `gemini-3-flash` *(placeholder â€” Claude on credit restore)* |
| `MODEL_ANALYSIS` | `gemini-3-pro-high` |
| `MODEL_REVIEW` | `gemini-3-pro-high` *(placeholder â€” Opus on credit restore)* |

---

### 8. âœ‚ï¸ Auto-Continue System

If JARVIS is cut off mid-sentence (no terminal punctuation at end of reply), a **FastAPI BackgroundTask** is spawned to:
1. Append the truncated reply to message history.
2. Ask the model to "continue exactly from where you stopped."
3. Push the continuation via `send_via_cli()` as a second message to the user.

---

### 9. ğŸ›¡ï¸ Sentinel (`sbs/sentinel/`)

A file-governance module that runs at boot. It enforces structural rules on the workspace â€” preventing accidental writes to protected paths and logging file events for audit.

---

### 10. ğŸ‘· Gentle Worker Loop

A background async loop that runs every **10 minutes** (when plugged in and CPU < 20%) to:
- `brain.prune_graph()` â€” Remove low-confidence or stale knowledge triples.
- `conflicts.prune_conflicts()` â€” Deduplicate conflict entries in the conflict graph.

---

## Service Port Map

| Service | Port | Technology |
|---|---|---|
| Core API Gateway | `8000` | FastAPI / Uvicorn |
| Antigravity Proxy (OAuth) | `8080` | OpenClaw built-in |
| Qdrant Vector DB | `6333` | Qdrant (OrbStack container) |
| Ollama (Mac â€” embeddings) | `11434` | Ollama |
| Ollama (Windows PC â€” Vault) | `11434` | Ollama (remote) |

---

## Data Flow: One Full Request (Happy Path)

```mermaid
sequenceDiagram
    participant U  as User (WhatsApp)
    participant FG as FloodGate
    participant Q  as TaskQueue
    participant G  as API Gateway
    participant SBS as SBS Orchestrator
    participant ME as Memory Engine
    participant DC as Dual Cognition
    participant TC as Traffic Cop
    participant LLM as Selected LLM

    U->>FG: "hey write me a python script"
    FG->>Q: batch + dedup â†’ enqueue
    Q->>G: process_message_pipeline()
    G->>SBS: on_message(user, ...) â†’ get_system_prompt()
    G->>ME: query("write python script", limit=5)
    ME-->>G: retrieved memories + graph context
    G->>DC: think(user_message, history)
    DC-->>G: inner_monologue + tension_level
    G->>TC: "CODING or CASUAL?"
    TC-->>G: "CODING"
    G->>LLM: call_ag_code(assembled_messages)
    LLM-->>G: code response
    G->>SBS: on_message(assistant, reply)
    G-->>U: reply + footer stats\n(tokens, model, context%)
```

---

## Repository Layout (Key Files)

```
workspace/
â”œâ”€â”€ sci_fi_dashboard/
â”‚   â”œâ”€â”€ api_gateway.py          # Core FastAPI app (1,188 lines)
â”‚   â”œâ”€â”€ memory_engine.py        # Hybrid RAG engine
â”‚   â”œâ”€â”€ sqlite_graph.py         # SQLite knowledge graph
â”‚   â”œâ”€â”€ toxic_scorer_lazy.py    # Lazy-loaded toxicity scorer
â”‚   â”œâ”€â”€ dual_cognition.py       # Inner monologue engine
â”‚   â”œâ”€â”€ retriever.py            # Qdrant + reranker utilities
â”‚   â”œâ”€â”€ persona.py              # Persona loading helpers
â”‚   â”œâ”€â”€ build_persona.py        # Static persona builder
â”‚   â”œâ”€â”€ conflict_resolver.py    # Conflict graph manager
â”‚   â”œâ”€â”€ smart_entity.py         # FlashText entity gate
â”‚   â”œâ”€â”€ state.py                # Runtime state container
â”‚   â”œâ”€â”€ gateway/                # Async pipeline (queue, flood, dedup, worker, sender)
â”‚   â””â”€â”€ sbs/                    # Soul-Brain Sync persona engine
â”‚       â”œâ”€â”€ orchestrator.py     # Top-level SBS coordinator
â”‚       â”œâ”€â”€ ingestion/          # ConversationLogger + RawMessage schema
â”‚       â”œâ”€â”€ processing/         # Realtime + Batch processors
â”‚       â”œâ”€â”€ profile/            # ProfileManager (JSON layer store)
â”‚       â”œâ”€â”€ injection/          # PromptCompiler
â”‚       â””â”€â”€ sentinel/           # File governance
â””â”€â”€ db/
    â”œâ”€â”€ model_orchestrator.py   # Standalone model routing helper
    â”œâ”€â”€ async_worker.py         # DB-layer async worker
    â””â”€â”€ ingest.py               # Fact ingestion pipeline
```

---

## Design Principles

| Principle | Implementation |
|---|---|
| **Zero-duplication singletons** | All core engines (graph, memory, toxicity) initialized once and shared |
| **Async-first** | Full asyncio stack; no blocking calls in the hot path |
| **Memory-optimized** | LazyToxicScorer auto-unloads; `OLLAMA_KEEP_ALIVE=0`; graph/conflict pruning on idle |
| **Zero cloud leakage for private sessions** | Spicy / private tasks routed to local Ollama Vault; never to cloud APIs |
| **Self-evolving persona** | SBS batch processor continuously rebuilds personality profile from conversation history |
| **Cost-aware routing** | Traffic Cop prevents simple greetings from hitting expensive models |
| **Resilient delivery** | Auto-Continue catches cut-off responses and pushes continuations asynchronously |
