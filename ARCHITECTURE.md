# ğŸ§  System Architecture â€” Project Phoenix

> A deep-dive into the modular, decentralized, and self-evolving design of JARVIS.

GitHub automatically renders the Mermaid diagrams below. If you are viewing this locally, use a Markdown viewer that supports Mermaid.js, or view it on GitHub.

---

## Architecture Diagram

![JARVIS â€” Project Phoenix Architecture](./architecture_diagram.png)

> *Full interactive diagram with annotations is available in the [Figma file](https://www.figma.com/@upayan). The sections below break down each subsystem in detail.*

---

## High-Level System Map

This diagram illustrates the full end-to-end flow: from user input, through the Async Gateway Pipeline, across the Cognitive Engine (MoA + Dual Cognition), and back out as a response.

```mermaid
flowchart LR
    %% â”€â”€ INPUTS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    subgraph Inputs["â‘  User Inputs"]
        WA["ğŸ“± WhatsApp Webhook\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nNode gateway\nPOST /webhook"]
        CLI["ğŸ’» OpenClaw CLI\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nDeveloper proxy\ndirect to gateway"]
    end

    %% â”€â”€ ASYNC PIPELINE â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    subgraph Async["â‘¡ Async Gateway Pipeline  (WhatsApp only)"]
        direction TB
        FG["ğŸ›¡ï¸ FloodGate\nBatches messages\nover a 3 s window"]
        DD["ğŸ” Deduplicator\nDrops duplicates\nwithin 5 min"]
        Q["ğŸ“¦ Task Queue\nHolds up to\n100 tasks"]
        W["âš™ï¸ Worker\n2 concurrent\ntasks"]
        FG --> DD --> Q --> W
    end

    %% â”€â”€ CORE GATEWAY â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    G(["ğŸš€ Core API Gateway\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nFastAPI / Uvicorn\n:8000"])

    %% â”€â”€ CONTEXT ENGINE â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    subgraph Brain["â‘¢ Context Engine  (bidirectional with Gateway)"]
        direction TB

        subgraph SBS["ğŸ­ Persona Engine â€” Soul-Brain Sync"]
            direction LR
            SBS_O["Orchestrator"] --- SBS_P["Profile\nManager"]
            SBS_O --- SBS_RT["Realtime\nProcessor"] --- SBS_B["Batch\nProcessor"]
            SBS_O --- SBS_C["Prompt\nCompiler"]
            SBS_P --- SBS_L["Conversation\nLogger"]
        end

        subgraph Mem["ğŸ’¾ Cognitive Memory"]
            direction LR
            ME["ğŸ§  Memory Engine\nHybrid Retrieval v3"]
            ME <--> M1["ğŸ—ƒï¸ SQLite\nGraph DB"]
            ME <--> M2["ğŸ”· Qdrant\nVector DB"]
            ME --> RE["ğŸ… FlashRank\nReranker"]
        end

        subgraph DC["ğŸ§© Dual Cognition"]
            direction LR
            DCE["DualCognitionEngine"] --- TS["â˜£ï¸ LazyToxicScorer\n(deferred tension check)"]
        end
    end

    %% â”€â”€ MIXTURE OF AGENTS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    subgraph MoA["â‘£ Mixture of Agents"]
        direction TB
        TC{"ğŸš¦ Traffic Cop\nIntent Classifier"}
        LLM1["ğŸŸ¢ Gemini 3 Flash\nCASUAL â€” everyday chat"]
        LLM2["ğŸ’» The Hacker\nCODING â€” code & debug"]
        LLM3["ğŸ›ï¸ The Architect\nANALYSIS â€” deep planning"]
        LLM4["ğŸ§ The Philosopher\nREVIEW â€” critical review"]
        LLM5["ğŸŒ¶ï¸ The Vault\nSPICY â€” local model"]

        TC -->|casual| LLM1
        TC -->|coding| LLM2
        TC -->|analysis| LLM3
        TC -->|review| LLM4
        TC -->|spicy| LLM5
    end

    %% â”€â”€ OUTPUT â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    subgraph Out["â‘¤ Output"]
        direction TB
        AC["âœ‚ï¸ Auto-Continue\nDetects cut-off responses\nand re-requests completion"]
        FO["ğŸ“¨ Final Output\nBack to WhatsApp\nor CLI caller"]
    end

    %% â”€â”€ CONNECTIONS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    WA -->|HTTP POST /webhook| FG
    CLI -->|CLI proxy| G
    W --> G

    G <-->|inject persona context| SBS_O
    G <-->|semantic + graph query| ME
    G -->|tension check| DCE

    G -->|classify intent| TC

    LLM1 & LLM2 & LLM3 & LLM4 & LLM5 -->|response + stats| G

    G --> AC
    G --> FO
    AC -.->|"re-requests if cut off"| G
```

---

## Component Breakdown

### 1. ğŸ“± Ingress Layer

| Input Channel               | Transport              | Handler                                            |
| --------------------------- | ---------------------- | -------------------------------------------------- |
| WhatsApp (via Node Gateway) | HTTP POST `/webhook` | `FloodGate` â†’ `Deduplicator` â†’ `TaskQueue` |
| OpenClaw CLI                | CLI Proxy subprocess   | Direct â†’`Core API Gateway`                      |

### 2. âš™ï¸ Async Gateway Pipeline (`workspace/sci_fi_dashboard/gateway/`)

Messages from WhatsApp enter an asynchronous multi-stage pipeline **before** hitting the cognitive engine. This prevents webhook timeouts and ensures ordered, deduplicated processing.

```mermaid
sequenceDiagram
    participant WA as WhatsApp Node
    participant FG as FloodGate (3s batch)
    participant DD as Deduplicator (5m)
    participant Q  as TaskQueue (100)
    participant W  as MessageWorker (Ã—2)
    participant G  as API Gateway

    WA->>FG: POST /whatsapp/enqueue
    FG->>FG: Batch rapid-fire messages
    FG->>DD: Flush batch
    DD->>DD: Check 5-min seen-set
    DD->>Q: Enqueue MessageTask
    Q->>W: Dequeue (FIFO)
    W->>G: process_message_pipeline()
    G-->>W: reply string
    W->>WA: send_via_cli()
```

| File                  | Role                                                                 |
| --------------------- | -------------------------------------------------------------------- |
| `gateway/queue.py`  | `TaskQueue` â€” asyncio-based FIFO, max 100 tasks                   |
| `gateway/flood.py`  | `FloodGate` â€” batches messages within a 3-second window           |
| `gateway/dedup.py`  | `MessageDeduplicator` â€” 5-minute seen-set for exact deduplication |
| `gateway/worker.py` | `MessageWorker` â€” 2 concurrent async workers consuming the queue  |
| `gateway/sender.py` | `WhatsAppSender` â€” wraps the OpenClaw CLI `send` command        |

---

### 3. ğŸš€ Core API Gateway (`api_gateway.py`)

The central FastAPI application running on **port 8000**. Every cognitive operation is orchestrated from here.

**API Routes:**

| Method   | Route                     | Description                                               |
| -------- | ------------------------- | --------------------------------------------------------- |
| `POST` | `/chat/the_creator`     | Chat endpoint for primary user (brother mode)             |
| `POST` | `/chat/the_partner`     | Chat endpoint for partner (caring PA mode)                |
| `POST` | `/chat`                 | Generic fallback (Banglish persona)                       |
| `POST` | `/whatsapp/enqueue`     | Async WhatsApp ingress entry point                        |
| `GET`  | `/whatsapp/status/{id}` | Poll status of an enqueued message                        |
| `POST` | `/persona/rebuild`      | Re-parse chat logs and rebuild persona profiles           |
| `GET`  | `/persona/status`       | Profile statistics and embedding mode                     |
| `POST` | `/ingest`               | Ingest a structured fact into the knowledge graph         |
| `POST` | `/add`                  | Unstructured memory â†’ LLM â†’ triple extraction           |
| `POST` | `/query`                | Query the knowledge graph                                 |
| `GET`  | `/health`               | System health check                                       |
| `GET`  | `/v1/models`            | OpenAI-compatible model list (for Node Gateway discovery) |
| `POST` | `/v1/chat/completions`  | OpenAI-compatible proxy endpoint                          |

**Singleton Modules (initialized once at boot):**

```python
brain          = SQLiteGraph()           # Knowledge graph
gate           = EntityGate(...)         # FlashText keyword extractor
conflicts      = ConflictManager(...)    # Conflict deduplication
toxic_scorer   = LazyToxicScorer(...)    # Lazy-loaded toxicity scorer
memory_engine  = MemoryEngine(...)       # Hybrid RAG engine
dual_cognition = DualCognitionEngine(...)# Inner monologue engine
```

---

### 4. ğŸ§  Cognitive Memory â€” Hybrid RAG (`memory_engine.py`, `sqlite_graph.py`, `retriever.py`)

Three-tier retrieval engine that provides grounded memory context before any LLM call.

```mermaid
graph LR
    Q[User Query] --> EE[Entity Extraction\nFlashText]
    EE --> GQ[Graph Query\nSQLite Triples]
    EE --> VQ[Vector Search\nQdrant + nomic-embed-text]
    GQ --> MERGE[Score Merge\na=0.7 semantic + b=0.1 temporal]
    VQ --> MERGE
    MERGE --> FG2{High Confidence\ngt 0.80?}
    FG2 -->|Yes| FAST[âš¡ Fast Gate\nReturn top-k directly]
    FG2 -->|No|  RR[(ğŸ… FlashRank Reranker\nms-marco-TinyBERT)]
    RR --> OUT[Ranked Context\nfor Prompt]
    FAST --> OUT
```

| Store                  | Technology             | Port       | Purpose                                 |
| ---------------------- | ---------------------- | ---------- | --------------------------------------- |
| `memory.db`          | SQLite                 | local file | Document store & embedding queue        |
| `knowledge_graph.db` | SQLite (graph)         | local file | Subjectâ€“Predicateâ€“Object triple store |
| Qdrant                 | Qdrant (native binary) | `:6333`  | High-speed semantic vector search       |

**Retrieval Tiers:**

1. **Fast Gate** â€” if â‰¥ `limit` results score > 0.80, return immediately (no reranker overhead).
2. **Reranked** â€” `FlashRank` (ms-marco-TinyBERT-L-2-v2) re-scores all Qdrant candidates for higher precision.

**Temporal Routing:**

- Queries containing words like `"was"`, `"history"`, `"2024"` â†’ `Î²=0.0` (pure semantic, no recency boost).
- Queries containing `"current"`, `"now"`, `"today"` â†’ `Î²=0.5` (blend recency with semantic).
- Default â†’ `Î²=0.1` (mild recency nudge).

---

### 5. ğŸ­ Soul-Brain Sync (SBS) Persona Engine (`sbs/`)

The SBS system is responsible for making JARVIS feel like a person, not a chatbot. It continuously tracks and evolves a structured **persona profile** for each conversation target in real-time.

```mermaid
graph TD
    MSG[Inbound Message] --> RT[Realtime Processor\nSentiment Â· Language Â· Mood]
    RT --> LOG[Conversation Logger\nSQLite]
    LOG --> CNT{50 msgs\nor 6h elapsed?}
    CNT -->|Yes| BATCH[Batch Processor\nProfile Rebuild]
    BATCH --> PM[(Profile Manager\nJSON Layers)]
    PM --> PC[Prompt Compiler\nSystem Prompt Assembly]
    PC --> SYS[Assembled System Prompt\nto LLM]
```

**Profile Layers tracked per target:**

| Layer               | Data captured                                                       |
| ------------------- | ------------------------------------------------------------------- |
| `emotional_state` | Dominant mood, sentiment average, mood trajectory                   |
| `linguistic`      | Banglish ratio, formality index, language mix                       |
| `vocabulary`      | Unique word count, preferred phrases, emoji frequency               |
| `meta`            | Total messages processed, last batch run timestamp, profile version |

**Two SBS instances run simultaneously:**

- `sbs_the_creator` â€” tuned for primary user (casual, direct, sibling-like)
- `sbs_the_partner` â€” tuned for the partner (warm, supportive, PA-like)

---

### 6. ğŸ§© Dual Cognition Engine (`dual_cognition.py`)

Before generating a reply, JARVIS thinks. The `DualCognitionEngine` generates an **inner monologue** and calculates a **tension level** to decide if there is emotional conflict between the retrieved memory and the current user request.

```mermaid
graph LR
    UM[User Message] --> DC[DualCognitionEngine.think]
    DC --> IM[Inner Monologue\nvia Gemini Flash]
    DC --> TL[Tension Level\n0.0 to 1.0]
    DC --> TT[Tension Type\nAMBIVALENT Â· CERTAIN Â· etc.]
    IM & TL & TT --> CC[Cognitive Context Block\ninjected into System Prompt]
```

The `LazyToxicScorer` is loaded alongside DualCognition. It auto-unloads after **30 seconds of idle** to save RAM â€” critical for a Mac Air host.

---

### 7. ğŸš¦ Mixture of Agents (MoA) Router

The **Traffic Cop** classifies every user message before routing it to the appropriate specialist model.

```mermaid
graph TD
    TC{Traffic Cop\nGemini Flash Classifier} -->|CASUAL|        A[ğŸŸ¢ AG_CASUAL\nGemini 3 Flash\nHigh throughput / free tier]
    TC -->|CODING|   B[ğŸ’» The Hacker\nClaude Sonnet 4.5\nMax logic depth]
    TC -->|ANALYSIS| C[ğŸ›ï¸ The Architect\nGemini 3 Pro\nLong-context synthesis]
    TC -->|REVIEW|   D[ğŸ§ The Philosopher\nClaude Opus 4.6\nNuanced critique]
    TC -->|SPICY session| E[ğŸŒ¶ï¸ The Vault\nStheno v3.2 on Ollama\nZero cloud footprint]
```

All cloud models route through the **Antigravity Proxy** (`localhost:8080`) using an OAuth token. The vault (Stheno) connects directly to a Windows PC Ollama instance (`WINDOWS_PC_IP:11434`).

**Model constants (configurable via env):**

| Constant           | Default Model                                                     |
| ------------------ | ----------------------------------------------------------------- |
| `MODEL_CASUAL`   | `gemini-3-flash`                                                |
| `MODEL_CODING`   | `gemini-3-flash` *(placeholder â€” Claude on credit restore)*  |
| `MODEL_ANALYSIS` | `gemini-3-pro-high`                                             |
| `MODEL_REVIEW`   | `gemini-3-pro-high` *(placeholder â€” Opus on credit restore)* |

---

### 8. âœ‚ï¸ Auto-Continue System

If JARVIS is cut off mid-sentence (no terminal punctuation at end of reply), a **FastAPI BackgroundTask** is spawned to:

1. Append the truncated reply to message history.
2. Ask the model to "continue exactly from where you stopped."
3. Push the continuation via `send_via_cli()` as a second message to the user.

---

### 9. ğŸ›¡ï¸ Sentinel (`sbs/sentinel/`)

A file-governance module that runs at boot. It enforces structural rules on the workspace â€” preventing accidental writes to protected paths and logging file events for audit.

---

### 10. ğŸ‘· Gentle Worker Loop

A background async loop that runs every **10 minutes** (when plugged in and CPU < 20%) to:

- `brain.prune_graph()` â€” Remove low-confidence or stale knowledge triples.
- `conflicts.prune_conflicts()` â€” Deduplicate conflict entries in the conflict graph.

---

## Service Port Map

| Service                      | Port      | Technology                  |
| ---------------------------- | --------- | --------------------------- |
| Core API Gateway             | `8000`  | FastAPI / Uvicorn           |
| Antigravity Proxy (OAuth)    | `8080`  | OpenClaw built-in           |
| Qdrant Vector DB             | `6333`  | Qdrant (OrbStack container) |
| Ollama (Mac â€” embeddings)   | `11434` | Ollama                      |
| Ollama (Windows PC â€” Vault) | `11434` | Ollama (remote)             |

---

## Data Flow: One Full Request (Happy Path)

```mermaid
sequenceDiagram
    participant U  as User (WhatsApp)
    participant FG as FloodGate
    participant Q  as TaskQueue
    participant G  as API Gateway
    participant SBS as SBS Orchestrator
    participant ME as Memory Engine
    participant DC as Dual Cognition
    participant TC as Traffic Cop
    participant LLM as Selected LLM

    U->>FG: "hey write me a python script"
    FG->>Q: batch + dedup â†’ enqueue
    Q->>G: process_message_pipeline()
    G->>SBS: on_message(user, ...) â†’ get_system_prompt()
    G->>ME: query("write python script", limit=5)
    ME-->>G: retrieved memories + graph context
    G->>DC: think(user_message, history)
    DC-->>G: inner_monologue + tension_level
    G->>TC: "CODING or CASUAL?"
    TC-->>G: "CODING"
    G->>LLM: call_ag_code(assembled_messages)
    LLM-->>G: code response
    G->>SBS: on_message(assistant, reply)
    G-->>U: reply + footer stats\n(tokens, model, context%)
```

---

## Repository Layout (Key Files)

```
workspace/
â”œâ”€â”€ sci_fi_dashboard/
â”‚   â”œâ”€â”€ api_gateway.py          # Core FastAPI app (1,188 lines)
â”‚   â”œâ”€â”€ memory_engine.py        # Hybrid RAG engine
â”‚   â”œâ”€â”€ sqlite_graph.py         # SQLite knowledge graph
â”‚   â”œâ”€â”€ toxic_scorer_lazy.py    # Lazy-loaded toxicity scorer
â”‚   â”œâ”€â”€ dual_cognition.py       # Inner monologue engine
â”‚   â”œâ”€â”€ retriever.py            # Qdrant + reranker utilities
â”‚   â”œâ”€â”€ persona.py              # Persona loading helpers
â”‚   â”œâ”€â”€ build_persona.py        # Static persona builder
â”‚   â”œâ”€â”€ conflict_resolver.py    # Conflict graph manager
â”‚   â”œâ”€â”€ smart_entity.py         # FlashText entity gate
â”‚   â”œâ”€â”€ state.py                # Runtime state container
â”‚   â”œâ”€â”€ gateway/                # Async pipeline (queue, flood, dedup, worker, sender)
â”‚   â””â”€â”€ sbs/                    # Soul-Brain Sync persona engine
â”‚       â”œâ”€â”€ orchestrator.py     # Top-level SBS coordinator
â”‚       â”œâ”€â”€ ingestion/          # ConversationLogger + RawMessage schema
â”‚       â”œâ”€â”€ processing/         # Realtime + Batch processors
â”‚       â”œâ”€â”€ profile/            # ProfileManager (JSON layer store)
â”‚       â”œâ”€â”€ injection/          # PromptCompiler
â”‚       â””â”€â”€ sentinel/           # File governance
â””â”€â”€ db/
    â”œâ”€â”€ model_orchestrator.py   # Standalone model routing helper
    â”œâ”€â”€ async_worker.py         # DB-layer async worker
    â””â”€â”€ ingest.py               # Fact ingestion pipeline
```

---

## Design Principles

| Principle                                         | Implementation                                                                          |
| ------------------------------------------------- | --------------------------------------------------------------------------------------- |
| **Zero-duplication singletons**             | All core engines (graph, memory, toxicity) initialized once and shared                  |
| **Async-first**                             | Full asyncio stack; no blocking calls in the hot path                                   |
| **Memory-optimized**                        | LazyToxicScorer auto-unloads;`OLLAMA_KEEP_ALIVE=0`; graph/conflict pruning on idle    |
| **Zero cloud leakage for private sessions** | Spicy / private tasks routed to local Ollama Vault; never to cloud APIs                 |
| **Self-evolving persona**                   | SBS batch processor continuously rebuilds personality profile from conversation history |
| **Cost-aware routing**                      | Traffic Cop prevents simple greetings from hitting expensive models                     |
| **Resilient delivery**                      | Auto-Continue catches cut-off responses and pushes continuations asynchronously         |
