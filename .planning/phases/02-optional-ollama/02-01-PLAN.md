---
phase: 02-optional-ollama
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - workspace/sci_fi_dashboard/memory_engine.py
  - workspace/scripts/nightly_ingest.py
  - requirements.txt
autonomous: true
requirements:
  - OLL-01
  - OLL-02
  - OLL-03

must_haves:
  truths:
    - "Importing memory_engine succeeds on a machine with no ollama package installed"
    - "get_embedding() returns 384-dim sentence-transformers vectors when Ollama is unavailable"
    - "think() does not raise AttributeError when OLLAMA_AVAILABLE is False"
    - "MemoryEngine.__init__ prints a clear message naming unavailable features and the dimension-mismatch risk when Ollama is absent"
    - "nightly_ingest.py exits cleanly with an actionable error message when Ollama is not installed"
  artifacts:
    - path: "workspace/sci_fi_dashboard/memory_engine.py"
      provides: "Module-level OLLAMA_AVAILABLE flag, _sentence_transformer_embed fallback, updated get_embedding and think methods, startup warning log"
      contains: "OLLAMA_AVAILABLE"
    - path: "workspace/scripts/nightly_ingest.py"
      provides: "Import guard + early exit with clear error when Ollama absent"
      contains: "HAS_OLLAMA"
    - path: "requirements.txt"
      provides: "ollama>=0.1.0 commented out as optional"
      contains: "# ollama"
  key_links:
    - from: "workspace/sci_fi_dashboard/memory_engine.py"
      to: "sentence_transformers.SentenceTransformer"
      via: "_sentence_transformer_embed() lazy import"
      pattern: "from sentence_transformers import SentenceTransformer"
    - from: "memory_engine.py get_embedding()"
      to: "OLLAMA_AVAILABLE flag"
      via: "if not OLLAMA_AVAILABLE: branch"
      pattern: "if not OLLAMA_AVAILABLE"
    - from: "memory_engine.py think()"
      to: "OLLAMA_AVAILABLE flag"
      via: "if OLLAMA_AVAILABLE: guard around ollama.chat()"
      pattern: "if OLLAMA_AVAILABLE"
---

<objective>
Guard the Ollama import at module level in memory_engine.py so the module loads on machines without Ollama installed, activate the sentence-transformers fallback embedding path, and add a startup log that names unavailable features and the dimension-mismatch risk.

Purpose: OLL-01/OLL-02/OLL-03 — the app currently crashes at import time on machines without Ollama because `import ollama` on line 64 of memory_engine.py is a bare module-level statement. This blocks all functionality before any function runs.

Output: memory_engine.py with OLLAMA_AVAILABLE flag, _sentence_transformer_embed() method, updated get_embedding() and think(), and startup warning; nightly_ingest.py with import guard; requirements.txt with ollama commented optional.
</objective>

<execution_context>
@C:/Users/upayan.ghosh/.claude/get-shit-done/workflows/execute-plan.md
@C:/Users/upayan.ghosh/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/02-optional-ollama/02-RESEARCH.md

<interfaces>
<!-- Key existing code. Executor must reference exact line numbers and patterns. -->

From workspace/skills/llm_router.py (already-correct pattern to replicate):
```python
try:
    import ollama
    HAS_OLLAMA = True
except ImportError:
    HAS_OLLAMA = False
```

From workspace/sci_fi_dashboard/memory_engine.py (current state — what to replace):
```python
# Line 64 — bare import to replace:
import ollama  # noqa: E402

# Lines 67-68 — configuration (leave as-is):
EMBEDDING_MODEL = "nomic-embed-text"
OLLAMA_KEEP_ALIVE = "0"

# Lines 80-95 — MemoryEngine.__init__ (add _st_model attr and startup log):
def __init__(self, graph_store=None, keyword_processor=None):
    self.qdrant_store = QdrantVectorStore()
    self.graph_store = graph_store
    self.keyword_processor = keyword_processor
    self._ranker = None
    self._ranker_lock = threading.Lock()
    print("[OK] MemoryEngine initialized (shared graph, no duplication)")

# Lines 97-108 — get_embedding (replace dead-code fallback with real fallback):
@lru_cache(maxsize=500)  # noqa: B019
def get_embedding(self, text: str) -> list:
    try:
        response = ollama.embeddings(...)
        return tuple(response["embedding"])
    except Exception as e:
        print(f"[WARN] Embedding generation failed: {e}")
        return tuple([0.0] * 768)  # dead-code zero-vector fallback -- replace this

# Lines 360-374 — think() local fallback (must guard with OLLAMA_AVAILABLE):
response = ollama.chat(
    model="llama3.2:3b",
    messages=[...],
    keep_alive=OLLAMA_KEEP_ALIVE,
)
```

From workspace/scripts/nightly_ingest.py (current state):
```python
# Line 8 — bare import to replace:
import ollama
```
</interfaces>
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add OLLAMA_AVAILABLE flag and fallback method to memory_engine.py</name>
  <files>workspace/sci_fi_dashboard/memory_engine.py</files>
  <action>
Make four targeted changes to memory_engine.py. Do NOT touch any other part of the file.

**Change 1 — Replace bare import on line 64:**
Replace:
```python
import ollama  # noqa: E402
```
With:
```python
try:
    import ollama
    OLLAMA_AVAILABLE = True
except ImportError:
    ollama = None
    OLLAMA_AVAILABLE = False
```

**Change 2 — Update MemoryEngine.__init__ (lines 80-95):**
Add `self._st_model = None` to the instance attributes block (after `self._ranker_lock = threading.Lock()`).
Replace the single print statement at the end with a conditional startup log:
```python
self._st_model = None  # lazy-loaded sentence-transformers fallback

if OLLAMA_AVAILABLE:
    print("[OK] MemoryEngine initialized (Ollama available -- nomic-embed-text)")
else:
    print("[WARN] Ollama not found -- local embedding and The Vault disabled")
    print("[WARN] Embedding fallback: sentence-transformers all-MiniLM-L6-v2 (384-dim)")
    print("[WARN] If DB has existing 768-dim vectors, re-ingest to restore semantic search")
print("[OK] MemoryEngine initialized (shared graph, no duplication)")
```

**Change 3 — Add _sentence_transformer_embed method and update get_embedding:**
Add the private method immediately before get_embedding (as a new method in the MemoryEngine class):
```python
def _sentence_transformer_embed(self, text: str) -> tuple:
    """Fallback embedding using all-MiniLM-L6-v2 (384-dim). Lazy-loaded."""
    if self._st_model is None:
        from sentence_transformers import SentenceTransformer
        self._st_model = SentenceTransformer("all-MiniLM-L6-v2")
    return tuple(self._st_model.encode(text).tolist())
```

Replace the body of get_embedding entirely:
```python
@lru_cache(maxsize=500)  # noqa: B019
def get_embedding(self, text: str) -> tuple:
    if not OLLAMA_AVAILABLE:
        return self._sentence_transformer_embed(text)
    try:
        response = ollama.embeddings(
            model=EMBEDDING_MODEL,
            prompt=text,
            keep_alive=OLLAMA_KEEP_ALIVE,
        )
        return tuple(response["embedding"])
    except Exception as e:
        print(f"[WARN] Embedding generation failed: {e}")
        return tuple([0.0] * 768)
```

**Change 4 — Guard think() local fallback (lines ~360-374):**
The `# Local fallback` block calls `ollama.chat()` unconditionally. Wrap it with `if OLLAMA_AVAILABLE:` and add an else branch:
```python
# Local fallback
if OLLAMA_AVAILABLE:
    response = ollama.chat(
        model="llama3.2:3b",
        messages=[
            {"role": "system", "content": system},
            {"role": "user", "content": prompt},
        ],
        keep_alive=OLLAMA_KEEP_ALIVE,
    )
    return {
        "response": response["message"]["content"],
        "model": "llama3.2:3b",
        "source": "local_fallback",
    }
else:
    return {"error": "Ollama not available and cloud LLM router unavailable"}
```

The outer `except Exception as e: return {"error": str(e)}` on line 374 stays unchanged.

**Critical pitfall to avoid:** Do NOT add a try/except at function level only — the crash is at module import time (line 64). The module-level guard in Change 1 is mandatory.
  </action>
  <verify>
    <automated>cd /c/Users/upayan.ghosh/personal/Jarvis-OSS && python -c "import sys; sys.path.insert(0, 'workspace'); from sci_fi_dashboard import memory_engine; print('OLLAMA_AVAILABLE:', memory_engine.OLLAMA_AVAILABLE)"</automated>
  </verify>
  <done>
    - `python -c "from sci_fi_dashboard import memory_engine"` succeeds regardless of whether ollama package is installed
    - `memory_engine.OLLAMA_AVAILABLE` is accessible as a module-level bool
    - MemoryEngine class has `_sentence_transformer_embed` method and `_st_model` attribute
    - `get_embedding()` routes to sentence-transformers when OLLAMA_AVAILABLE is False
    - `think()` local fallback block is guarded with `if OLLAMA_AVAILABLE:`
  </done>
</task>

<task type="auto">
  <name>Task 2: Guard nightly_ingest.py and comment out ollama in requirements.txt</name>
  <files>workspace/scripts/nightly_ingest.py</files>
  <files>requirements.txt</files>
  <action>
**nightly_ingest.py — Replace bare import on line 8:**
Replace:
```python
import ollama
```
With:
```python
import sys as _sys

try:
    import ollama
    HAS_OLLAMA = True
except ImportError:
    HAS_OLLAMA = False

if not HAS_OLLAMA:
    print("[ERROR] nightly_ingest.py requires Ollama. Install from https://ollama.com")
    _sys.exit(1)
```

Note: `sys` may already be imported elsewhere in the file — check before adding. If `import sys` already exists at the top of the file, use the existing `sys` reference and skip the `import sys as _sys` line. Just add the try/except and the sys.exit block.

**requirements.txt — Comment out the ollama line:**
Find the line `ollama>=0.1.0` (or similar) and replace it with:
```
# --- Optional: Local LLM (Ollama CLI must be installed separately from https://ollama.com) ---
# ollama>=0.1.0
```
The Python ollama client package installs cleanly on any machine (it is a small HTTP wrapper); the problem is the Ollama CLI server binary not being present. Commenting it out makes the dependency explicit-optional rather than auto-installed. Do NOT remove it entirely — users who want Ollama can uncomment it.
  </action>
  <verify>
    <automated>cd /c/Users/upayan.ghosh/personal/Jarvis-OSS && grep -n "HAS_OLLAMA\|import ollama" workspace/scripts/nightly_ingest.py && grep -n "ollama" requirements.txt</automated>
  </verify>
  <done>
    - nightly_ingest.py: `import ollama` replaced with try/except + HAS_OLLAMA flag + sys.exit(1) when unavailable
    - requirements.txt: `ollama>=0.1.0` commented out with explanatory note
    - `grep "^import ollama\|^ollama>=" workspace/scripts/nightly_ingest.py requirements.txt` returns zero results (bare imports eliminated)
  </done>
</task>

</tasks>

<verification>
Run these checks after both tasks complete:

1. Import succeeds without ollama package:
   ```bash
   cd /c/Users/upayan.ghosh/personal/Jarvis-OSS/workspace
   python -c "from sci_fi_dashboard import memory_engine; print('OK, OLLAMA_AVAILABLE:', memory_engine.OLLAMA_AVAILABLE)"
   ```

2. Confirm all ollama call sites are guarded:
   ```bash
   grep -n "ollama\." workspace/sci_fi_dashboard/memory_engine.py
   ```
   Every match should be inside `if OLLAMA_AVAILABLE:` or `try:` blocks. No bare `ollama.` at module level.

3. Confirm requirements.txt change:
   ```bash
   grep "ollama" requirements.txt
   ```
   Should show only commented lines.

4. Run existing tests (smoke):
   ```bash
   cd /c/Users/upayan.ghosh/personal/Jarvis-OSS/workspace && pytest tests/ -m smoke -v 2>/dev/null || echo "No smoke tests found"
   ```
</verification>

<success_criteria>
- `python -c "from sci_fi_dashboard import memory_engine"` in workspace/ directory: no exception
- `memory_engine.OLLAMA_AVAILABLE` is a bool (True or False depending on environment)
- MemoryEngine() instance can be created without Ollama installed
- When OLLAMA_AVAILABLE is False: startup prints three [WARN] lines naming unavailable features and dimension-mismatch risk
- `get_embedding("test")` returns a non-zero tuple (via sentence-transformers) when Ollama unavailable
- nightly_ingest.py prints [ERROR] and exits 1 when Ollama absent (does not crash with traceback)
- requirements.txt has no uncommented `ollama>=` line
</success_criteria>

<output>
After completion, create `.planning/phases/02-optional-ollama/02-01-SUMMARY.md` using the summary template.
</output>
