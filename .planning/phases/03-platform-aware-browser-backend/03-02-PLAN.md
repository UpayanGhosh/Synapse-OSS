---
phase: 03-platform-aware-browser-backend
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - workspace/db/tools.py
  - workspace/scrape_threads.py
autonomous: true
requirements:
  - BRW-04

must_haves:
  truths:
    - "tools.py can be imported on Windows without ModuleNotFoundError (no bare crawl4ai import at module level)"
    - "On Windows, ToolRegistry.search_web(url) dispatches to the Playwright backend and returns a non-empty string"
    - "On Mac/Linux, ToolRegistry.search_web(url) dispatches to crawl4ai and returns a non-empty string (behaviour unchanged)"
    - "get_tool_schemas() method and search_web(url) signature are unchanged"
    - "scrape_threads.py prints a clear error and exits on Windows instead of crashing with ModuleNotFoundError"
  artifacts:
    - path: "workspace/db/tools.py"
      provides: "Platform-aware browser dispatch"
      exports: ["ToolRegistry"]
      contains: "sys.platform == \"win32\""
    - path: "workspace/scrape_threads.py"
      provides: "Windows-safe crawl4ai guard"
      contains: "sys.platform"
  key_links:
    - from: "workspace/db/tools.py"
      to: "playwright.async_api"
      via: "lazy import inside _search_web_playwright on Windows"
      pattern: "from playwright.async_api import async_playwright"
    - from: "workspace/db/tools.py"
      to: "crawl4ai.AsyncWebCrawler"
      via: "lazy import inside _search_web_crawl4ai on Mac/Linux"
      pattern: "from crawl4ai import AsyncWebCrawler"
---

<objective>
Rewrite workspace/db/tools.py so the `search_web(url)` interface dispatches to Playwright on Windows and Crawl4AI on Mac/Linux using a `sys.platform` check and lazy imports. Guard workspace/scrape_threads.py so it exits cleanly on Windows instead of crashing.

Purpose: The bare `from crawl4ai import AsyncWebCrawler` at line 3 of tools.py causes ModuleNotFoundError on Windows when any module imports ToolRegistry. This is the root cause of the runtime crash even after requirements.txt is fixed. scrape_threads.py has the same bare import and will confuse Windows users who run it directly.

Output: tools.py with platform-aware dispatch; scrape_threads.py with Windows guard.
</objective>

<execution_context>
@C:/Users/upayan.ghosh/.claude/get-shit-done/workflows/execute-plan.md
@C:/Users/upayan.ghosh/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@workspace/db/tools.py
@workspace/scrape_threads.py

<interfaces>
<!-- Current tools.py — full content for reference before rewrite -->

Current workspace/db/tools.py (42 lines):
```python
import asyncio
import json
from crawl4ai import AsyncWebCrawler   # <-- BARE MODULE-LEVEL IMPORT — must be removed

class ToolRegistry:
    @staticmethod
    def get_tool_schemas():
        """Returns the OpenAI-compatible JSON schema for the tools."""
        return [
            {
                "type": "function",
                "function": {
                    "name": "search_web",
                    "description": "Searches the live internet by visiting a URL and extracting the main content as clean markdown. Use this whenever you need up-to-date information, weather, or factual data not in your memory.",
                    "parameters": {
                        "type": "object",
                        "properties": {
                            "url": {
                                "type": "string",
                                "description": "The exact URL to visit (e.g., 'https://lite.cnn.com' or a google search URL like 'https://www.google.com/search?q=weather+kolkata')"
                            }
                        },
                        "required": ["url"]
                    }
                }
            }
        ]

    @staticmethod
    async def search_web(url: str) -> str:
        """The actual execution engine for the browser."""
        print(f"[Crawl4AI] Navigating to: {url}...")
        try:
            async with AsyncWebCrawler() as crawler:
                result = await crawler.arun(url=url)
                # CRITICAL: Truncate to 3000 chars to protect your 8GB VRAM context limit
                clean_text = result.markdown[:3000]
                print(f"[Crawl4AI] Successfully extracted {len(clean_text)} characters.")
                return clean_text
        except Exception as e:
            return f"Error accessing {url}: {str(e)}"
```

Current workspace/scrape_threads.py (15 lines):
```python
import asyncio
import sys
from crawl4ai import AsyncWebCrawler   # <-- BARE MODULE-LEVEL IMPORT — must be guarded

async def main():
    url = "https://www.threads.net/@talonhayess/post/DU_DKLfktBv"
    async with AsyncWebCrawler() as crawler:
        result = await crawler.arun(url=url)
        if result.success:
            print(result.markdown)
        else:
            print(f"Error: {result.error_message}")

if __name__ == "__main__":
    asyncio.run(main())
```
</interfaces>
</context>

<tasks>

<task type="auto">
  <name>Task 1: Rewrite tools.py with platform-aware browser dispatch</name>
  <files>workspace/db/tools.py</files>
  <action>
Replace the entire content of workspace/db/tools.py with the following. The public interface (`get_tool_schemas()` and `search_web(url: str) -> str`) is unchanged. Only the internal implementation gains platform dispatch.

Key requirements:
- Remove the bare `from crawl4ai import AsyncWebCrawler` at the top (line 3)
- Add `import sys` at the top
- Keep `import asyncio` and `import json`
- `search_web()` dispatches to `_search_web_playwright()` on Windows, `_search_web_crawl4ai()` on all other platforms
- Both `_search_web_crawl4ai` and `_search_web_playwright` use lazy imports (import inside the function body) — never at module level
- Playwright backend: use `page.inner_text("body")` not `page.content()` — returns visible text without HTML tags (no extra dependency needed)
- Playwright timeout: 15000ms (matches the 3000-char truncation philosophy — quick lookups, not crawls)
- Both backends truncate to 3000 chars
- `[Crawl4AI]` and `[Playwright]` prefixes on print statements to identify which backend ran

Complete replacement content:

```python
import asyncio
import json
import sys


class ToolRegistry:
    @staticmethod
    def get_tool_schemas():
        """Returns the OpenAI-compatible JSON schema for the tools."""
        return [
            {
                "type": "function",
                "function": {
                    "name": "search_web",
                    "description": "Searches the live internet by visiting a URL and extracting the main content as clean markdown. Use this whenever you need up-to-date information, weather, or factual data not in your memory.",
                    "parameters": {
                        "type": "object",
                        "properties": {
                            "url": {
                                "type": "string",
                                "description": "The exact URL to visit (e.g., 'https://lite.cnn.com' or a google search URL like 'https://www.google.com/search?q=weather+kolkata')",
                            }
                        },
                        "required": ["url"],
                    },
                },
            }
        ]

    @staticmethod
    async def search_web(url: str) -> str:
        """The actual execution engine for the browser.

        Dispatches to Playwright on Windows (sys.platform == 'win32') and
        Crawl4AI on Mac/Linux. Both backends return up to 3000 chars of
        visible page text.
        """
        if sys.platform == "win32":
            return await ToolRegistry._search_web_playwright(url)
        else:
            return await ToolRegistry._search_web_crawl4ai(url)

    @staticmethod
    async def _search_web_crawl4ai(url: str) -> str:
        from crawl4ai import AsyncWebCrawler  # lazy import -- only on Mac/Linux

        print(f"[Crawl4AI] Navigating to: {url}...")
        try:
            async with AsyncWebCrawler() as crawler:
                result = await crawler.arun(url=url)
                # CRITICAL: Truncate to 3000 chars to protect your 8GB VRAM context limit
                clean_text = result.markdown[:3000]
                print(f"[Crawl4AI] Successfully extracted {len(clean_text)} characters.")
                return clean_text
        except Exception as e:
            return f"Error accessing {url}: {str(e)}"

    @staticmethod
    async def _search_web_playwright(url: str) -> str:
        from playwright.async_api import async_playwright  # lazy import -- only on Windows

        print(f"[Playwright] Navigating to: {url}...")
        try:
            async with async_playwright() as p:
                browser = await p.chromium.launch(headless=True)
                page = await browser.new_page()
                await page.goto(url, timeout=15000)
                # inner_text("body") returns visible text -- closest equivalent to crawl4ai markdown output
                # CRITICAL: Truncate to 3000 chars to protect your 8GB VRAM context limit
                text = await page.inner_text("body")
                clean_text = text[:3000]
                await browser.close()
                print(f"[Playwright] Successfully extracted {len(clean_text)} characters.")
                return clean_text
        except Exception as e:
            return f"Error accessing {url}: {str(e)}"
```

Note: `sys.platform == "win32"` is correct for ALL Windows versions (32-bit and 64-bit). This is a Python historical artifact — do not use `"win64"` or `platform.system() == "Windows"`.
  </action>
  <verify>
    <automated>cd C:/Users/upayan.ghosh/personal/Jarvis-OSS && python -c "import sys; sys.path.insert(0, 'workspace'); from db.tools import ToolRegistry; print('Import OK'); schemas = ToolRegistry.get_tool_schemas(); assert schemas[0]['function']['name'] == 'search_web'; print('Schema OK')"</automated>
  </verify>
  <done>
- `from db.tools import ToolRegistry` succeeds on the current platform (Windows) without ModuleNotFoundError
- `ToolRegistry.get_tool_schemas()` returns the same schema as before (search_web function with url parameter)
- No bare `from crawl4ai import` at module level: `grep -n "^from crawl4ai" workspace/db/tools.py` returns empty
- `grep "sys.platform" workspace/db/tools.py` shows the dispatch check
  </done>
</task>

<task type="auto">
  <name>Task 2: Guard scrape_threads.py against Windows crawl4ai import crash</name>
  <files>workspace/scrape_threads.py</files>
  <action>
Replace the entire content of workspace/scrape_threads.py. The file is a standalone utility (not imported by any module), so the fix is a simple early exit on Windows with a clear message, followed by the Mac/Linux implementation unchanged.

The bare `from crawl4ai import AsyncWebCrawler` at line 3 must be removed from module level. Since this is a script (not a library), the cleanest guard is an early sys.platform check that exits with a message before the import is attempted.

Complete replacement content:

```python
import asyncio
import sys

if sys.platform == "win32":
    print("[scrape_threads] This script uses crawl4ai which is not available on Windows.")
    print("                 On Windows, use the /browse endpoint via Playwright instead.")
    sys.exit(1)

from crawl4ai import AsyncWebCrawler  # only reached on Mac/Linux


async def main():
    url = "https://www.threads.net/@talonhayess/post/DU_DKLfktBv"
    async with AsyncWebCrawler() as crawler:
        result = await crawler.arun(url=url)
        if result.success:
            print(result.markdown)
        else:
            print(f"Error: {result.error_message}")


if __name__ == "__main__":
    asyncio.run(main())
```

This pattern is correct because:
- The `if sys.platform == "win32": sys.exit(1)` runs before Python reaches the `from crawl4ai import` line
- No ModuleNotFoundError is raised on Windows
- On Mac/Linux, the script behaves exactly as before
- The error message is actionable — tells the user what to use instead
  </action>
  <verify>
    <automated>cd C:/Users/upayan.ghosh/personal/Jarvis-OSS && grep -n "^from crawl4ai" workspace/scrape_threads.py && echo "FAIL: bare import still at module level" || echo "PASS: no bare module-level crawl4ai import"</automated>
  </verify>
  <done>
- `grep "^from crawl4ai" workspace/scrape_threads.py` returns empty (no bare module-level crawl4ai import)
- `grep "sys.platform" workspace/scrape_threads.py` shows the Windows guard
- Running `python workspace/scrape_threads.py` on Windows prints the "not available on Windows" message and exits with code 1 (not ModuleNotFoundError)
  </done>
</task>

</tasks>

<verification>
1. `python -c "import sys; sys.path.insert(0, 'workspace'); from db.tools import ToolRegistry"` — no error
2. `grep "^from crawl4ai" workspace/db/tools.py` — empty (no bare module-level import)
3. `grep "^from crawl4ai" workspace/scrape_threads.py` — empty (import is inside function or guarded)
4. `grep "sys.platform" workspace/db/tools.py` — shows dispatch check
5. `grep "_search_web_playwright\|_search_web_crawl4ai" workspace/db/tools.py` — both methods present
</verification>

<success_criteria>
- tools.py: No bare top-level crawl4ai import; `ToolRegistry` imports cleanly on Windows; `search_web()` dispatches to correct backend based on `sys.platform`; both backends truncate to 3000 chars; `get_tool_schemas()` unchanged
- scrape_threads.py: No bare top-level crawl4ai import; exits with clear message on Windows; works unchanged on Mac/Linux
</success_criteria>

<output>
After completion, create `.planning/phases/03-platform-aware-browser-backend/03-02-SUMMARY.md` with:
- The complete new content of tools.py (or a diff showing what changed)
- The complete new content of scrape_threads.py
- Output of the import verification command
- Note confirming get_tool_schemas() schema is unchanged
</output>
